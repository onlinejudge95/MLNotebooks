{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pipelining a Supervised learning model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Library imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/maypatha/Python/PyCon/2018/Liza.Sander-ML.in.scraping/venv/lib/python3.7/site-packages/sklearn/ensemble/weight_boosting.py:29: DeprecationWarning: numpy.core.umath_tests is an internal NumPy module and should not be imported. It will be removed in a future NumPy release.\n",
      "  from numpy.core.umath_tests import inner1d\n"
     ]
    }
   ],
   "source": [
    "from civismlext.stacking import StackedClassifier\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.ensemble import GradientBoostingClassifier, RandomForestClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import Imputer\n",
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Loading the data set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = load_iris(return_X_y=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Simple example using LogisticRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2 1 0 1 0 0 1 0 2 2 0 1 2 0 0 1 0 0 1 1 1 0 1 0 2 1 1 2 2 1 1 0 1 1 2 0 1\n",
      " 0]\n"
     ]
    }
   ],
   "source": [
    "train_X, test_X, train_y, test_y = train_test_split(X, y)\n",
    "model = LogisticRegression(solver='lbfgs', multi_class='ovr')\n",
    "model.fit(train_X, train_y)\n",
    "scores = model.predict(test_X)\n",
    "print(scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using Pipelines for ETL-like jobs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 0 2 2 1 0 1 2 1 2 0 0 0 0 2 0 2 1 0 0 2 1 0 1 0 2 1 1 1 1 0 0 1 1 2 2 1\n",
      " 2]\n"
     ]
    }
   ],
   "source": [
    "train_X, test_X, train_y, test_y = train_test_split(X, y)\n",
    "\n",
    "estimator_list = [\n",
    "    ('scaler', StandardScaler()),\n",
    "    ('logistic', LogisticRegression(solver='lbfgs', multi_class='ovr')),\n",
    "]\n",
    "\n",
    "pipeline = Pipeline(estimator_list)\n",
    "pipeline.fit(train_X, train_y)\n",
    "scores = pipeline.predict(test_X)\n",
    "print(scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using ensemble learning methodlogies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2 1 2 2 0 1 2 1 2 0 2 2 1 2 0 2 0 1 0 2 2 0 1 0 1 0 2 1 1 2 2 1 0 0 0 1 0\n",
      " 2]\n",
      "[('scaler', StandardScaler(copy=True, with_mean=True, with_std=True)), ('custom_estimator', GradientBoostingClassifier(criterion='friedman_mse', init=None,\n",
      "              learning_rate=0.1, loss='deviance', max_depth=3,\n",
      "              max_features=None, max_leaf_nodes=None,\n",
      "              min_impurity_decrease=0.0, min_impurity_split=None,\n",
      "              min_samples_leaf=1, min_samples_split=2,\n",
      "              min_weight_fraction_leaf=0.0, n_estimators=50,\n",
      "              presort='auto', random_state=None, subsample=1.0, verbose=0,\n",
      "              warm_start=False))]\n"
     ]
    }
   ],
   "source": [
    "def score_iris(estimator):\n",
    "    X, y = load_iris(return_X_y=True)\n",
    "    train_X, test_X, train_y, test_y = train_test_split(X, y)\n",
    "    estimator_list = [\n",
    "        ('scaler', StandardScaler()),\n",
    "        ('custom_estimator', estimator),\n",
    "    ]\n",
    "    \n",
    "    pipeline = Pipeline(estimator_list)\n",
    "    pipeline.fit(train_X, train_y)\n",
    "    scores = pipeline.predict(test_X)\n",
    "    return pipeline, scores\n",
    "\n",
    "\n",
    "model = GradientBoostingClassifier(n_estimators=50)\n",
    "pipeline, scores = score_iris(model)\n",
    "print(scores)\n",
    "print(pipeline.steps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pipeline(memory=None,\n",
      "     steps=[('scaler', StandardScaler(copy=True, with_mean=True, with_std=True)), ('custom_estimator', Pipeline(memory=None,\n",
      "     steps=[('imputer', Imputer(axis=0, copy=True, missing_values='NaN', strategy='mean', verbose=0)), ('gradient_boosting_classifier', GradientBoostingClassifier(criterion='friedm...   presort='auto', random_state=None, subsample=1.0, verbose=0,\n",
      "              warm_start=False))]))])\n"
     ]
    }
   ],
   "source": [
    "new_estimator_list = [\n",
    "    ('imputer', Imputer()),\n",
    "    ('gradient_boosting_classifier', GradientBoostingClassifier()),\n",
    "]\n",
    "pipeline_estimator = Pipeline(new_estimator_list)\n",
    "pipeline, scores = score_iris(pipeline_estimator)\n",
    "print(pipeline)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using StackedClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 2 2 2 2 2 2 2 2 2 2 2\n",
      " 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2\n",
      " 2 2]\n"
     ]
    }
   ],
   "source": [
    "estimator_list = [\n",
    "    ('logistic', LogisticRegression(solver='lbfgs', multi_class='ovr')),\n",
    "    ('random_forest', RandomForestClassifier(n_estimators=10)),\n",
    "    ('gradient_boosting_classifier', GradientBoostingClassifier()),\n",
    "    ('meta', LogisticRegression(solver='lbfgs', multi_class='ovr')),\n",
    "]\n",
    "\n",
    "stacker = StackedClassifier(estimator_list)\n",
    "stacker.fit(X, y)\n",
    "scores = stacker.predict(X)\n",
    "print(scores)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
